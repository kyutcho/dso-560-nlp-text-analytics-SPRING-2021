{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3\n",
    "\n",
    "Submit via Slack. Due on Tuesday, April 13th, 2020, 6:29pm PST. You may work with one other person.\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "You are an analyst working at McDonalds as a store operations analyst, and charged with identifying areas for improvement for each franchise. Several metropolitan locations have been suffering recently from lower reviews.\n",
    "\n",
    "Using the **mcdonalds-yelp-negative-reviews.csv** dataset, clean and parse the text reviews. Explain the decisions you make:\n",
    "- why remove/keep stopwords?\n",
    "- which stopwords to remove?\n",
    "- stemming versus lemmatization?\n",
    "- regex cleaning and substitution?\n",
    "- adding in custom stopwords?\n",
    "- what `n` for your `n-grams`?\n",
    "- which words to collocate together?\n",
    "\n",
    "Finally, generate a TF-IDF report that either **visualizes** or explains for a business (non-technical) stakeholder:\n",
    "* the features your analysis showed that customers cited as reasons for a poor review\n",
    "* the most common issues identified from your analysis that generated customer dissatisfaction.\n",
    "\n",
    "Explain to what degree the TF-IDF findings make sense - what are its limitations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac_df = pd.read_csv(\"mcdonalds-yelp-negative-reviews.csv\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all lowercase\n",
    "mac_df[\"review\"] = mac_df[\"review\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\", binary=False)\n",
    "\n",
    "X = vectorizer.fit_transform(mac_df[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names()).T\n",
    "\n",
    "vec_df[\"num_count\"] = vec_df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df.sort_values(\"num_count\", ascending=False)\\\n",
    "    .head(20)\n",
    "#     .drop(list(set(stopwords.words('english'))), errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the CountVectorizer, we can notice that many reviews contains \"food\", \"order\", \"service\", and \"time\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamburger Variation\n",
    "mac_df[\"review\"] = mac_df[\"review\"].str.replace(r\"\\w*\\s*burgers?\", \"burger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big Macs\n",
    "mac_df[\"review\"] = mac_df[\"review\"].str.replace(r\"big\\s*macs?\", \"burger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change types of burgers into burger to see how burgers served are reviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# McDonald's\n",
    "mac_df[\"review\"] = mac_df[\"review\"].str.replace(r\"(?:\\bmcdonald(?:'?s?)?\\b)|(?:\\bmcds?\\b)\", \"mcdonald\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changed variations of McDonald's to mcdonald to add to stopwords later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation Removal\n",
    "mac_df[\"review\"] = mac_df[\"review\"].str.replace(r\"[!|@|#|$|%|^|&|*|(|)|+|<|>|?|:|.|,|;|\\\"|\\'|\\\\]\", ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whitespace\n",
    "mac_df[\"review\"] = mac_df[\"review\"].str.replace(r\"\\s{2,}\", ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers\n",
    "mac_df[\"review\"] = mac_df[\"review\"].str.replace(r\"\\d+\\S*\\d*\\w*\", \"NUM_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miscellaneous Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stmmer_func(review):\n",
    "    tokens = [stemmer.stem(token) for token in review.split()]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason for choosing SnowballStemmer: Stemming is more appropriate than Lemmetization in our analysis because we want to search for keywords that causes the bad reviews. For example, we want to know if the order was the issue for the customer. And words \"ordered\" and \"order\" are likely to indicate that the review has something to do with order. Also, I chose SnowballStemmer over PorterStemmer because it is the improved version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Add stopword to stopwords list\n",
    "stop_words.append(\"mcdonald\")\n",
    "stop_words = stop_words + [\".\",'.', \",\",\":\", \"''\", \"'s\", \"'\", \"``\", \"(\", \")\", \"-\", \"!\", \"*\", \"?\"]\n",
    "\n",
    "# Remove from stopwords list\n",
    "# stop_words.remove([\"because\", \"most\", \"only\"])\n",
    "\n",
    "for w in [\"because\", \"most\", \"only\"]:\n",
    "    stop_words.remove(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"because\" in stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I removed \"mcdonald\" as we already know the review is about mcdonald. \\\n",
    "I added because the word(s) after \"because\" signals what causes customers to leave a bad review. Same logic applies to \"most\" and \"only\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# https://gaurav5430.medium.com/using-nltk-for-lemmatizing-sentences-c1bfff963258\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "mac_df[\"tokenized_words\"] = mac_df[\"review\"].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words in stopwords\n",
    "mac_df[\"tokenized_words\"] = mac_df[\"tokenized_words\"].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocation_finder = BigramCollocationFinder.from_documents(mac_df[\"tokenized_words\"])\n",
    "measures = BigramAssocMeasures()\n",
    "\n",
    "collocation_finder.nbest(measures.raw_freq, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the words that tend to collocate together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input=\"mcdonalds-yelp-negative-reviews.csv\",\n",
    "                         encoding=\"latin1\",\n",
    "                         lowercase=True,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(mac_df[\"review\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tf_idf.sum(axis=1)\n",
    "score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram justification: I tried bigram for the analysis, but most words with high scores are usually collocated words like drive thru and ice cream. And quadgram simply has too many words that it does not appear in top rows. That is why I chose trigram. I acknowledge treating collocated words into one word will improve the analysis much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the bad reviews are order related. Customers either did not get what they ordered or it took too long to get what they ordered, or even both. It seems like drive through and customer service need improvements as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is consistent with the CountVectorizer analysis above as words like order, service were frequent words. However, none of tokens with high TF-IDF scores are food related as opposed to what we have observed in CounterVectorizer. There may be many reasons for that, but my assumption is that food is weighted down as they appear in many reviews. Also, there might be many variations around the word food, and thus not show up as much as the single word \"food\" does.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Attribution (Feature Engineering and Regex Practice)\n",
    "\n",
    "Download the [dataset](https://dso-560-nlp-text-analytics.s3.amazonaws.com/truncated_catalog.csv) from the class S3 bucket (`dso560-nlp-text-analytics`).\n",
    "\n",
    "In preparation for the group project, our client company has provided a dataset of women's clothing products they are considering cataloging. \n",
    "\n",
    "1. Filter for only **women's clothing items**.\n",
    "\n",
    "2. For each clothing item:\n",
    "\n",
    "* Identify its **category**:\n",
    "```\n",
    "Bottom\n",
    "One Piece\n",
    "Shoe\n",
    "Handbag\n",
    "Scarf\n",
    "```\n",
    "* Identify its **color**:\n",
    "```\n",
    "Beige\n",
    "Black\n",
    "Blue\n",
    "Brown\n",
    "Burgundy\n",
    "Gold\n",
    "Gray\n",
    "Green\n",
    "Multi \n",
    "Navy\n",
    "Neutral\n",
    "Orange\n",
    "Pinks\n",
    "Purple\n",
    "Red\n",
    "Silver\n",
    "Teal\n",
    "White\n",
    "Yellow\n",
    "```\n",
    "\n",
    "Your output will be the same dataset, except with **3 additional fields**:\n",
    "* `is_womens_clothing`\n",
    "* `product_category`\n",
    "* `colors`\n",
    "\n",
    "`colors` should be a list of colors, since it is possible for a piece of clothing to have multiple colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df = pd.read_csv(\"truncated_catalog.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = prod_df.columns.to_list()\n",
    "\n",
    "for col in cols:\n",
    "    prod_df[col] = prod_df[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Null Values\n",
    "prod_df.iloc[:, :7] = prod_df.iloc[:, :7].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is_womens_clothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex for capturing women related words \n",
    "woman_exp = \"\\bwi(?:fe|ves)|girls?|wom(?:a|e)n|lad(?:y|ies)|madams?|brides?|widows?|females?|femini\\w*|maternal\\w*|moms?\\b\"\n",
    "\n",
    "# Search all columns\n",
    "for col in cols:\n",
    "    prod_df[f\"is_womens_clothing_{col}\"] = False\n",
    "    \n",
    "    # Find if women related words exist in the column\n",
    "    prod_df[f\"is_womens_clothing_{col}\"] = prod_df[col].str.contains(woman_exp, case=False, flags=re.IGNORECASE, regex=True)\n",
    "        \n",
    "    print(f\"{col} searched\")\n",
    "    \n",
    "# If any of is_womens_clothing is True, then is_womens_clothing is True. Otherwise False\n",
    "prod_df[f\"is_womens_clothing\"] = prod_df.iloc[:, 7:].any(axis=1)\n",
    "\n",
    "# Drop other intermediate columns\n",
    "col_to_drop = prod_df.iloc[0, 7:-1].index.to_list()\n",
    "prod_df.drop(col_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## product_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expressions\n",
    "bottom_exp = \"(?:baggies|bottom|pant|jean|cord|chino|denim|legging|overall|short|trouser)(?:es|s)?\"\n",
    "one_piece_exp = \"\\bone[\\S|\\s]?piece|\\w*dress|all[\\S|\\s]?in[\\S|\\s]?one\\b\"\n",
    "shoe_exp = \"(?:shoe|boot|cleat|hopper|trainer|flat|flip[\\S|\\s]?flop|heel|pump|slide|slipper|skate|sneaker|wedge)(?:s|es)?\"\n",
    "handbag_exp = \"(?:\\w* ?bags?|clutch(?:es)?|satchels?)\"\n",
    "scarf_exp = \"(?:\\w* ?scar(?:f|(?:ves))?|snoods?|stoles?|boas?|sarongs?)\"\n",
    "\n",
    "cats_list = [\"Bottom\", \"One_Piece\", \"Shoe\", \"Handbag\", \"Scarf\"]\n",
    "exps_list = [bottom_exp, one_piece_exp, shoe_exp, handbag_exp, scarf_exp]\n",
    "\n",
    "# For each product category\n",
    "for cat, exp in zip(cats_list, exps_list):\n",
    "    prod_df.loc[:,cat] = 0\n",
    "    \n",
    "    for col in cols:\n",
    "        # Add the number of occurrences in all columns\n",
    "        prod_df[cat] = prod_df[col].str.findall(exp, flags=re.IGNORECASE).apply(lambda x: len(x))\n",
    "        print(f\"{cat} in {col} searched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the category that has the highest score\n",
    "prod_df[\"product_category\"] = prod_df.iloc[:, 8:].apply(lambda x: x.idxmax() if x.sum() != 0 else None, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgement: idxmax fails to identify the category when there is a tie. idxmax fails to break the tie as it chooses the index of former tie. For example, if both shoe and handbag show up once in a product, it idxmax will choose shoe instead of handbag, which may not be true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df.drop([\"Bottom\", \"One_Piece\", \"Shoe\", \"Handbag\", \"Scarf\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_exp = \"(?:Beige|Black|Blue|Brown|Burgund(?:y|ies)|Gold|Gra(?:y|ies)|Green|Multi|Nav(?:y|ies)|Neutral|Orange|Pink|Purple|Red|Silver|Teal|White|Yellow)s?\"\n",
    "\n",
    "for col in cols:\n",
    "    prod_df[f\"colors_{col}\"] = None\n",
    "    \n",
    "    # Find colors\n",
    "    prod_df[f\"colors_{col}\"] = prod_df[col].str.findall(color_exp, flags=re.IGNORECASE).apply(lambda x: ''.join(x))\n",
    "        \n",
    "    print(f\"{col} searched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append colors to the list\n",
    "prod_df[\"colors\"] = prod_df.iloc[:, 9:].apply(lambda x: list(set([color for color in x if color != ''])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "prod_df.drop([\"colors_brand\", \"colors_name\", \"colors_description\", \"colors_brand_category\",\n",
    "              \"colors_brand_canonical_url\", \"colors_details\", \"colors_tsv\"],\n",
    "             axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change color to \"Multi\" if multiple colors in a product\n",
    "prod_df[\"colors\"] = prod_df[\"colors\"].apply(lambda x: x if len(x) < 2 else [\"Multi\"])\n",
    "prod_df[\"colors\"] = prod_df[\"colors\"].apply(lambda x: None if len(x) == 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
